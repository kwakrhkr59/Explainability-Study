{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter \n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Flatten\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, Dropout, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Function is used to preprocess user tweets.\n",
    "    Removes @username and links, and whitespaces.\n",
    "    Args:\n",
    "        tweet (str): Raw text of the tweet.\n",
    "    Returns:\n",
    "        result (str): Processed tweet, after removing unnecessary data.\n",
    "    \"\"\"\n",
    "\n",
    "    # removing @username\n",
    "    result = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    # removing link\n",
    "    result = re.sub(r'https?://[A-Za-z0-9./]+', '', result)\n",
    "    # removing leading and trailing whitespace\n",
    "    result = result.strip()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def convert_label(polarity):\n",
    "    \"\"\"Simple function to preprocess polarity.\n",
    "    \"\"\"\n",
    "\n",
    "    if polarity == 4:\n",
    "        return 1\n",
    "    elif polarity == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        print('[WARNING]')\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Loads the data.\n",
    "    Function loads in data, preprocesses the data\n",
    "    (removes @username and links, and converts sentiment\n",
    "    into neg: 0 and pos: 1)\n",
    "    Returns:\n",
    "        texts (pd.Series): Preprocessed texts.\n",
    "        sentiment (pd.Series): Preprocessed sentiments.\n",
    "    \"\"\"\n",
    "\n",
    "    train_dir = '..\\\\data\\\\text\\\\train.csv'\n",
    "    columns = ['Polarity', 'ID', 'Date', 'Query', 'User', 'Texts']\n",
    "\n",
    "    df = pd.read_csv(train_dir, encoding='latin-1', names=columns, header=None)\n",
    "    df.drop(['ID', 'Date', 'Query', 'User'], axis=1, inplace=True)\n",
    "\n",
    "    sentiment_raw = df['Polarity']\n",
    "    sentiment = sentiment_raw.apply(lambda x: convert_label(x))\n",
    "\n",
    "    texts_raw = df['Texts']\n",
    "    texts = texts_raw.apply(lambda x: clean_text(x))\n",
    "\n",
    "    return texts, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 1590446290 created\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "MAX_SEQ_LEN = 40\n",
    "EMB_DIM = 100\n",
    "BASE_DIR = str(int(np.ceil(time.time())))\n",
    "\n",
    "if not os.path.exists(BASE_DIR):\n",
    "    os.makedirs(BASE_DIR)\n",
    "    print(f'[INFO] {BASE_DIR} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data...\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] Loading data...')\n",
    "texts, sentiments = load_data()\n",
    "\n",
    "train_texts, val_texts, train_sent, val_sent = train_test_split(texts, sentiments, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tk.fit_on_texts(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Number of unique tokens found (in train data): 283625\n"
     ]
    }
   ],
   "source": [
    "with open(f'{BASE_DIR}\\\\tokenizer.pickle', 'wb') as f:\n",
    "    pickle.dump(tk, f)\n",
    "\n",
    "word_index = tk.word_index\n",
    "print('[INFO] Number of unique tokens found (in train data):', len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tk.texts_to_sequences(train_texts)\n",
    "x_test = tk.texts_to_sequences(val_texts)\n",
    "\n",
    "max_length = len(max(x_train, key=len))\n",
    "if max_length > MAX_SEQ_LEN:\n",
    "    max_length = MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Sequence Length: 40\n",
      "[INFO] Shape of x_train: (1280000, 40)\n",
      "[INFO] Shape of y_train: (1280000, 1)\n",
      "[INFO] Shape of x_test: (320000, 40)\n",
      "[INFO] Shape of y_test: (320000, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = pad_sequences(x_test, maxlen=max_length)\n",
    "y_train = np.array(train_sent).reshape(-1, 1)\n",
    "y_test = np.array(val_sent).reshape(-1, 1)\n",
    "\n",
    "print(f'[INFO] Sequence Length: {max_length}')\n",
    "print(f'[INFO] Shape of x_train: {x_train.shape}')\n",
    "print(f'[INFO] Shape of y_train: {y_train.shape}')\n",
    "print(f'[INFO] Shape of x_test: {x_test.shape}')\n",
    "print(f'[INFO] Shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Glove Word Embedding\n",
    "Download glove embedding from : https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "File Name: glove.6B.100d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Indexing word vectors...\n",
      "[INFO] Total number of word vectors in Glove Embedding: 400000\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] Indexing word vectors...')\n",
    "embeddings_index = {}\n",
    "embedding_path = f'.\\glove.6B.{EMB_DIM}d.txt'\n",
    "\n",
    "with open(embedding_path, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('[INFO] Total number of word vectors in Glove Embedding:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preparing embedding matrix...\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] Preparing embedding matrix...')\n",
    "num_words = min(VOCAB_SIZE, len(word_index) + 1)\n",
    "embeddings_matrix = np.zeros((num_words, EMB_DIM))  # initializing zeros matrix\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= VOCAB_SIZE:\n",
    "        continue\n",
    "\n",
    "    embedding_vector = embeddings_index.get(word)  # vector for that word\n",
    "    if embedding_vector is not None:  # if word not found, then 0\n",
    "        embeddings_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vikra\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(num_words, EMB_DIM,\n",
    "                            weights=[embeddings_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=True, name = 'Embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vikra\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\vikra\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Embedding (Embedding)        (None, 40, 100)           2000000   \n",
      "_________________________________________________________________\n",
      "Drop_1 (Dropout)             (None, 40, 100)           0         \n",
      "_________________________________________________________________\n",
      "Conv_1 (Conv1D)              (None, 40, 512)           154112    \n",
      "_________________________________________________________________\n",
      "Max_1 (MaxPooling1D)         (None, 20, 512)           0         \n",
      "_________________________________________________________________\n",
      "Drop_2 (Dropout)             (None, 20, 512)           0         \n",
      "_________________________________________________________________\n",
      "Conv_2 (Conv1D)              (None, 20, 256)           393472    \n",
      "_________________________________________________________________\n",
      "Drop_3 (Dropout)             (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "Conv_3 (Conv1D)              (None, 20, 15)            11535     \n",
      "_________________________________________________________________\n",
      "Flatten_1 (Flatten)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 20)                6020      \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 2,565,160\n",
      "Trainable params: 2,565,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(max_length,), name = 'Input'))\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.2, name = 'Drop_1'))\n",
    "model.add(Conv1D(512, 3, activation='relu', padding='same', name = 'Conv_1'))\n",
    "model.add(MaxPooling1D(2, name = 'Max_1'))\n",
    "model.add(Dropout(0.3, name = 'Drop_2'))\n",
    "\n",
    "model.add(Conv1D(256, 3, activation='relu', padding='same', name = 'Conv_2'))\n",
    "model.add(Dropout(0.3, name = 'Drop_3'))\n",
    "\n",
    "model.add(Conv1D(15, 3, activation='relu', padding='same', name = 'Conv_3'))\n",
    "model.add(Flatten(name='Flatten_1'))\n",
    "model.add(Dense(20, name = 'Dense_2'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid', name = 'Output'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1280000 samples, validate on 320000 samples\n",
      "Epoch 1/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.4434 - acc: 0.7912\n",
      "Epoch 00001: val_acc improved from -inf to 0.82209, saving model to 1590446290\\best_Twitter_1590447138\n",
      "1280000/1280000 [==============================] - 64s 50us/sample - loss: 0.4434 - acc: 0.7912 - val_loss: 0.3964 - val_acc: 0.8221\n",
      "Epoch 2/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.3971 - acc: 0.8207- ETA: - ETA: 5s - loss: 0.3975 - acc: 0.8 - ETA: 4s - loss: 0.3976 - - ETA: 4s - loss: 0.397 - ETA: 3s - loss: 0. - ETA: 1s - \n",
      "Epoch 00002: val_acc improved from 0.82209 to 0.82761, saving model to 1590446290\\best_Twitter_1590447138\n",
      "1280000/1280000 [==============================] - 60s 47us/sample - loss: 0.3970 - acc: 0.8207 - val_loss: 0.3837 - val_acc: 0.8276\n",
      "Epoch 3/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8289\n",
      "Epoch 00003: val_acc improved from 0.82761 to 0.83019, saving model to 1590446290\\best_Twitter_1590447138\n",
      "1280000/1280000 [==============================] - 62s 48us/sample - loss: 0.3821 - acc: 0.8289 - val_loss: 0.3792 - val_acc: 0.8302\n",
      "Epoch 4/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8346\n",
      "Epoch 00004: val_acc improved from 0.83019 to 0.83173, saving model to 1590446290\\best_Twitter_1590447138\n",
      "1280000/1280000 [==============================] - 64s 50us/sample - loss: 0.3712 - acc: 0.8346 - val_loss: 0.3748 - val_acc: 0.8317\n",
      "Epoch 5/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.3625 - acc: 0.8392\n",
      "Epoch 00005: val_acc improved from 0.83173 to 0.83380, saving model to 1590446290\\best_Twitter_1590447138\n",
      "1280000/1280000 [==============================] - 65s 51us/sample - loss: 0.3625 - acc: 0.8392 - val_loss: 0.3727 - val_acc: 0.8338\n",
      "Epoch 6/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.8432- ETA: 4s - loss: - ETA: 2s - loss: 0.3545 - ETA: 1s - lo\n",
      "Epoch 00006: val_acc improved from 0.83380 to 0.83430, saving model to 1590446290\\best_Twitter_1590447138\n",
      "1280000/1280000 [==============================] - 65s 51us/sample - loss: 0.3546 - acc: 0.8432 - val_loss: 0.3715 - val_acc: 0.8343\n",
      "Epoch 7/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.3475 - acc: 0.8466\n",
      "Epoch 00007: val_acc improved from 0.83430 to 0.83513, saving model to 1590446290\\best_Twitter_1590447138\n",
      "1280000/1280000 [==============================] - 65s 51us/sample - loss: 0.3475 - acc: 0.8466 - val_loss: 0.3717 - val_acc: 0.8351\n",
      "Epoch 8/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.3409 - acc: 0.8502- ETA: 0s - loss: 0.3409 - a\n",
      "Epoch 00008: val_acc did not improve from 0.83513\n",
      "1280000/1280000 [==============================] - 65s 51us/sample - loss: 0.3408 - acc: 0.8502 - val_loss: 0.3746 - val_acc: 0.8344\n",
      "Epoch 9/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.3349 - acc: 0.8530\n",
      "Epoch 00009: val_acc improved from 0.83513 to 0.83557, saving model to 1590446290\\best_Twitter_1590447138\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.3349 - acc: 0.8531 - val_loss: 0.3726 - val_acc: 0.8356\n",
      "Epoch 10/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.3283 - acc: 0.8564\n",
      "Epoch 00010: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 65s 51us/sample - loss: 0.3283 - acc: 0.8564 - val_loss: 0.3753 - val_acc: 0.8346\n",
      "Epoch 11/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.3229 - acc: 0.8592\n",
      "Epoch 00011: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 65s 51us/sample - loss: 0.3229 - acc: 0.8592 - val_loss: 0.3745 - val_acc: 0.8340\n",
      "Epoch 12/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.8615- ETA: 9s - loss: 0.3176 - a - ETA: 9s - loss: 0.3175 -\n",
      "Epoch 00012: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.3179 - acc: 0.8615 - val_loss: 0.3809 - val_acc: 0.8329\n",
      "Epoch 13/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.8642 - ETA: 3s - loss: 0.31 - ETA: 2s - - ETA: 0s - loss: 0.3127 - acc: \n",
      "Epoch 00013: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.3128 - acc: 0.8642 - val_loss: 0.3844 - val_acc: 0.8337\n",
      "Epoch 14/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.3077 - acc: 0.8666- ETA: 2s - loss: 0.3075 -  - ETA: 1s - loss: 0.3076 - acc - ETA: 0s - loss: 0.3076 - acc: 0.866 - ETA: 0s - loss: 0.3076 - \n",
      "Epoch 00014: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.3077 - acc: 0.8666 - val_loss: 0.3817 - val_acc: 0.8325\n",
      "Epoch 15/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.3034 - acc: 0.8686\n",
      "Epoch 00015: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.3034 - acc: 0.8686 - val_loss: 0.3922 - val_acc: 0.8320\n",
      "Epoch 16/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.8707- ETA: 1s - loss: 0. - ETA: 0s - loss: 0.2989 - ac\n",
      "Epoch 00016: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2989 - acc: 0.8707 - val_loss: 0.3881 - val_acc: 0.8324\n",
      "Epoch 17/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.8730\n",
      "Epoch 00017: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 74s 58us/sample - loss: 0.2946 - acc: 0.8730 - val_loss: 0.3987 - val_acc: 0.8318\n",
      "Epoch 18/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2906 - acc: 0.8749\n",
      "Epoch 00018: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2906 - acc: 0.8750 - val_loss: 0.3953 - val_acc: 0.8319\n",
      "Epoch 19/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2869 - acc: 0.8768\n",
      "Epoch 00019: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2868 - acc: 0.8768 - val_loss: 0.3985 - val_acc: 0.8306\n",
      "Epoch 20/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.8782- ETA: 0s - loss: 0.2830 - acc: 0.878\n",
      "Epoch 00020: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2830 - acc: 0.8782 - val_loss: 0.3986 - val_acc: 0.8310\n",
      "Epoch 21/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2797 - acc: 0.8801- ETA: 2s - ETA: 0s - loss: 0.2796 - acc: 0\n",
      "Epoch 00021: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2797 - acc: 0.8801 - val_loss: 0.4057 - val_acc: 0.8290\n",
      "Epoch 22/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2762 - acc: 0.8815- ETA: - ETA: 1s - loss: 0.276\n",
      "Epoch 00022: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2762 - acc: 0.8815 - val_loss: 0.4078 - val_acc: 0.8303\n",
      "Epoch 23/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2729 - acc: 0.8831- ETA: 1s - loss: 0\n",
      "Epoch 00023: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2729 - acc: 0.8830 - val_loss: 0.4088 - val_acc: 0.8284\n",
      "Epoch 24/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2699 - acc: 0.8844\n",
      "Epoch 00024: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2699 - acc: 0.8844 - val_loss: 0.4170 - val_acc: 0.8282\n",
      "Epoch 25/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.8861- ETA: 0s - loss: 0.2668 - \n",
      "Epoch 00025: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2670 - acc: 0.8861 - val_loss: 0.4092 - val_acc: 0.8272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2643 - acc: 0.8873\n",
      "Epoch 00026: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2643 - acc: 0.8873 - val_loss: 0.4335 - val_acc: 0.8288\n",
      "Epoch 27/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2614 - acc: 0.8887\n",
      "Epoch 00027: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2615 - acc: 0.8887 - val_loss: 0.4154 - val_acc: 0.8272\n",
      "Epoch 28/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.8897- ETA: 9s - loss - ETA: 8s - los - ETA: - ETA: 4s - loss: 0.2585 - ac - ETA: 0s - loss: 0.2588 - acc\n",
      "Epoch 00028: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2589 - acc: 0.8897 - val_loss: 0.4295 - val_acc: 0.8267\n",
      "Epoch 29/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.8908\n",
      "Epoch 00029: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2567 - acc: 0.8908 - val_loss: 0.4302 - val_acc: 0.8262\n",
      "Epoch 30/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.8918- ETA: 1s - lo\n",
      "Epoch 00030: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2542 - acc: 0.8918 - val_loss: 0.4353 - val_acc: 0.8269\n",
      "Epoch 31/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2519 - acc: 0.8929\n",
      "Epoch 00031: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2519 - acc: 0.8929 - val_loss: 0.4372 - val_acc: 0.8257\n",
      "Epoch 32/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.8936- ETA: 6s - loss: 0.2495 - acc:  - ETA: 3s - loss: 0.250 - E\n",
      "Epoch 00032: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2501 - acc: 0.8936 - val_loss: 0.4436 - val_acc: 0.8257\n",
      "Epoch 33/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.8946- ETA: 5s - loss: 0.24 - ETA: 4s - loss: 0.2479  - ETA: 3s - loss: 0.2479 - acc: 0.8 - ETA: 3s - loss: 0.24 - ETA: 2s - los - ETA: 0s - loss: 0.2482 - acc: 0 - ETA: 0s - loss: 0.2482 - ac\n",
      "Epoch 00033: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2483 - acc: 0.8946 - val_loss: 0.4381 - val_acc: 0.8245\n",
      "Epoch 34/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.8954\n",
      "Epoch 00034: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2463 - acc: 0.8954 - val_loss: 0.4501 - val_acc: 0.8251\n",
      "Epoch 35/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.8968- ETA: 1s - loss: 0.2433 - acc: 0 - ETA: 0s - loss: 0.2432 - acc:  - ETA: 0s - loss: 0.2434 - acc:\n",
      "Epoch 00035: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2434 - acc: 0.8968 - val_loss: 0.4580 - val_acc: 0.8239\n",
      "Epoch 36/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.8975-\n",
      "Epoch 00036: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2424 - acc: 0.8975 - val_loss: 0.4527 - val_acc: 0.8253\n",
      "Epoch 37/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.8984- ETA: 1s - loss: 0.\n",
      "Epoch 00037: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2401 - acc: 0.8984 - val_loss: 0.4540 - val_acc: 0.8241\n",
      "Epoch 38/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2384 - acc: 0.8992- ETA: 0s - loss: 0.2384 - ac\n",
      "Epoch 00038: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2384 - acc: 0.8992 - val_loss: 0.4584 - val_acc: 0.8240\n",
      "Epoch 39/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.899 - ETA: 0s - loss: 0.2367 - acc: 0.8996\n",
      "Epoch 00039: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2367 - acc: 0.8996 - val_loss: 0.4679 - val_acc: 0.8233\n",
      "Epoch 40/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9004- ETA: 1s - los\n",
      "Epoch 00040: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 67s 52us/sample - loss: 0.2351 - acc: 0.9004 - val_loss: 0.4691 - val_acc: 0.8221\n",
      "Epoch 41/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9011\n",
      "Epoch 00041: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2340 - acc: 0.9011 - val_loss: 0.4543 - val_acc: 0.8235\n",
      "Epoch 42/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9021- ETA: 3s - loss: 0.2318  - ETA: 2s - lo - ETA: 1s\n",
      "Epoch 00042: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2321 - acc: 0.9021 - val_loss: 0.4676 - val_acc: 0.8232\n",
      "Epoch 43/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9022- ETA:\n",
      "Epoch 00043: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2311 - acc: 0.9022 - val_loss: 0.4722 - val_acc: 0.8217\n",
      "Epoch 44/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2291 - acc: 0.9034\n",
      "Epoch 00044: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 67s 52us/sample - loss: 0.2291 - acc: 0.9034 - val_loss: 0.4805 - val_acc: 0.8223\n",
      "Epoch 45/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9038\n",
      "Epoch 00045: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2277 - acc: 0.9038 - val_loss: 0.4733 - val_acc: 0.8217\n",
      "Epoch 46/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9046- ETA: 1s - loss\n",
      "Epoch 00046: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2268 - acc: 0.9046 - val_loss: 0.4583 - val_acc: 0.8224\n",
      "Epoch 47/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9052\n",
      "Epoch 00047: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2253 - acc: 0.9052 - val_loss: 0.4665 - val_acc: 0.8214\n",
      "Epoch 48/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9054- ETA: 1s - loss: 0.22\n",
      "Epoch 00048: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2243 - acc: 0.9054 - val_loss: 0.4812 - val_acc: 0.8215\n",
      "Epoch 49/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9060   - ETA\n",
      "Epoch 00049: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2230 - acc: 0.9060 - val_loss: 0.5040 - val_acc: 0.8214\n",
      "Epoch 50/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9067- ETA: - ETA: 5s - loss: 0.2210 - a - ETA: 4s - loss: 0.2212  - ETA: 0s - loss: 0.2215 -\n",
      "Epoch 00050: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2216 - acc: 0.9067 - val_loss: 0.4778 - val_acc: 0.8205\n",
      "Epoch 51/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2206 - acc: 0.9075- E\n",
      "Epoch 00051: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2206 - acc: 0.9075 - val_loss: 0.4856 - val_acc: 0.8217\n",
      "Epoch 52/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9074- ETA: 0s - loss: 0.2195 - acc\n",
      "Epoch 00052: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2196 - acc: 0.9074 - val_loss: 0.5007 - val_acc: 0.8210\n",
      "Epoch 53/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9082- ETA: 7\n",
      "Epoch 00053: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2184 - acc: 0.9082 - val_loss: 0.5161 - val_acc: 0.8205\n",
      "Epoch 54/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9084- ETA: 7s - loss: 0.2167 - acc: 0.9 - ETA: 7s -  - ETA: 5s - loss - ETA: 3s - loss: 0.2170 - acc: 0.9 - ETA: 3s - loss: 0.2170 - acc: 0.9 - ETA: 3s - loss: 0.2171 - acc:  - ETA: 3s - - ETA: 1s - loss: 0.2174 - acc: 0.9 - ETA: 1s - loss: 0.2174  - ETA: 0s - loss: 0.2175 - acc: 0.9\n",
      "Epoch 00054: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2176 - acc: 0.9084 - val_loss: 0.4915 - val_acc: 0.8203\n",
      "Epoch 55/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9090\n",
      "Epoch 00055: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2161 - acc: 0.9090 - val_loss: 0.4973 - val_acc: 0.8199\n",
      "Epoch 56/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9095- ETA: 4s  - ETA:  - ETA: 0s - loss: 0.2154 - \n",
      "Epoch 00056: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2154 - acc: 0.9095 - val_loss: 0.4874 - val_acc: 0.8209\n",
      "Epoch 57/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9098- ETA: 0s - loss: 0.2146 - acc: 0.909 - ETA: 0s - loss: 0.2146 - acc\n",
      "Epoch 00057: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2147 - acc: 0.9098 - val_loss: 0.5009 - val_acc: 0.8200\n",
      "Epoch 58/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9105- ETA: 0s - loss: 0.213\n",
      "Epoch 00058: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2134 - acc: 0.9105 - val_loss: 0.5030 - val_acc: 0.8191\n",
      "Epoch 59/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9105- ETA: 7s - loss: 0.2125 - a -  - ETA: 1s - loss: 0.2\n",
      "Epoch 00059: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2128 - acc: 0.9105 - val_loss: 0.5014 - val_acc: 0.8198\n",
      "Epoch 60/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9110- ETA: 0s - loss: 0.2120 - acc: 0. - ETA: 0s - loss: 0.2120 - acc:\n",
      "Epoch 00060: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2121 - acc: 0.9110 - val_loss: 0.5071 - val_acc: 0.8201\n",
      "Epoch 61/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9115- ETA: 3s - loss: 0.2\n",
      "Epoch 00061: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2108 - acc: 0.9115 - val_loss: 0.5113 - val_acc: 0.8195\n",
      "Epoch 62/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9120- ETA: 8s - loss: 0.2096 - acc - ETA: 7s - loss: 0.2096 - acc - ETA: 1s - loss:\n",
      "Epoch 00062: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2101 - acc: 0.9120 - val_loss: 0.5013 - val_acc: 0.8196\n",
      "Epoch 63/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9123- ETA: 2s - - ETA: 0s - loss: 0.2095 - acc\n",
      "Epoch 00063: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2096 - acc: 0.9123 - val_loss: 0.5036 - val_acc: 0.8204\n",
      "Epoch 64/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2085 - acc: 0.9128\n",
      "Epoch 00064: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2085 - acc: 0.9128 - val_loss: 0.5109 - val_acc: 0.8188\n",
      "Epoch 65/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9130  ETA: 11s - loss: 0.2069 - a - ETA: 11s - loss - ETA: 10s - loss: - ETA: 1s - loss: 0.2075 - acc: 0. - ETA: 1s - loss: 0.2075 -  - ETA: 0s - loss: 0.2077 - acc: 0.9 - ETA: 0s - loss: 0.2077 - acc\n",
      "Epoch 00065: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2077 - acc: 0.9130 - val_loss: 0.5064 - val_acc: 0.8196\n",
      "Epoch 66/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9134- ETA: 8s - loss: 0.2063 - acc - ETA: 7s - loss: 0.2 - ETA: 6s - loss: 0 - ETA: 4\n",
      "Epoch 00066: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2067 - acc: 0.9134 - val_loss: 0.5278 - val_acc: 0.8188\n",
      "Epoch 67/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9137- \n",
      "Epoch 00067: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2060 - acc: 0.9137 - val_loss: 0.5243 - val_acc: 0.8197\n",
      "Epoch 68/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9137\n",
      "Epoch 00068: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2057 - acc: 0.9137 - val_loss: 0.5294 - val_acc: 0.8190\n",
      "Epoch 69/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9143- ETA: 7s - loss: 0.2042 - acc: 0.91 - ET - \n",
      "Epoch 00069: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2046 - acc: 0.9143 - val_loss: 0.5560 - val_acc: 0.8189\n",
      "Epoch 70/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9145\n",
      "Epoch 00070: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2045 - acc: 0.9145 - val_loss: 0.5229 - val_acc: 0.8182\n",
      "Epoch 71/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2034 - acc: 0.9151- ETA: 0s - loss: 0.2034 - acc: 0.91\n",
      "Epoch 00071: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2034 - acc: 0.9151 - val_loss: 0.5164 - val_acc: 0.8173\n",
      "Epoch 72/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9152\n",
      "Epoch 00072: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 67s 52us/sample - loss: 0.2027 - acc: 0.9152 - val_loss: 0.5242 - val_acc: 0.8179\n",
      "Epoch 73/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2024 - acc: 0.9153\n",
      "Epoch 00073: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2024 - acc: 0.9153 - val_loss: 0.5390 - val_acc: 0.8184\n",
      "Epoch 74/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2013 - acc: 0.9158\n",
      "Epoch 00074: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2013 - acc: 0.9158 - val_loss: 0.5194 - val_acc: 0.8178\n",
      "Epoch 75/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9161- ETA: 1s - loss: 0.2\n",
      "Epoch 00075: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2008 - acc: 0.9161 - val_loss: 0.5236 - val_acc: 0.8173\n",
      "Epoch 76/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9164\n",
      "Epoch 00076: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.2002 - acc: 0.9164 - val_loss: 0.5119 - val_acc: 0.8181\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9165- ETA: 3 - ETA: 1s - loss: 0.200 - ETA: 0s - loss: 0.2000 - acc: 0\n",
      "Epoch 00077: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.2001 - acc: 0.9165 - val_loss: 0.5244 - val_acc: 0.8180\n",
      "Epoch 78/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9170- ETA: 7s - loss - ETA: 3s - loss: 0.1986 - acc: \n",
      "Epoch 00078: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.1990 - acc: 0.9170 - val_loss: 0.5413 - val_acc: 0.8175\n",
      "Epoch 79/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9170\n",
      "Epoch 00079: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.1988 - acc: 0.9170 - val_loss: 0.5304 - val_acc: 0.8177\n",
      "Epoch 80/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9171  ETA: 12s - loss: 0.1977 - - ETA - ETA: 11s - loss: 0.1979 - - ETA: 6s - loss: 0.19 - ETA\n",
      "Epoch 00080: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.1985 - acc: 0.9171 - val_loss: 0.5492 - val_acc: 0.8174\n",
      "Epoch 81/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9175\n",
      "Epoch 00081: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.1976 - acc: 0.9174 - val_loss: 0.5420 - val_acc: 0.8177\n",
      "Epoch 82/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.1967 - acc: 0.9178\n",
      "Epoch 00082: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.1967 - acc: 0.9178 - val_loss: 0.5516 - val_acc: 0.8185\n",
      "Epoch 83/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9184- ETA - ETA: 3s - loss: 0. - ETA: 1s -\n",
      "Epoch 00083: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.1958 - acc: 0.9184 - val_loss: 0.5677 - val_acc: 0.8175\n",
      "Epoch 84/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9181- ETA: 0s - loss: 0.1962 - acc: 0.9 - ETA: 0s - loss: 0.1961 - acc: 0.9181\n",
      "Epoch 00084: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.1962 - acc: 0.9181 - val_loss: 0.5608 - val_acc: 0.8176\n",
      "Epoch 85/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9185\n",
      "Epoch 00085: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.1954 - acc: 0.9185 - val_loss: 0.5468 - val_acc: 0.8178\n",
      "Epoch 86/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9185\n",
      "Epoch 00086: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.1952 - acc: 0.9185 - val_loss: 0.5356 - val_acc: 0.8181\n",
      "Epoch 87/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9187- ETA: 1s - loss: 0.\n",
      "Epoch 00087: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.1946 - acc: 0.9187 - val_loss: 0.5450 - val_acc: 0.8168\n",
      "Epoch 88/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9189\n",
      "Epoch 00088: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.1942 - acc: 0.9189 - val_loss: 0.5277 - val_acc: 0.8164\n",
      "Epoch 89/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9192- ETA: 1s - loss: 0.1939 - acc - ETA: 1s - loss: 0.193\n",
      "Epoch 00089: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.1940 - acc: 0.9192 - val_loss: 0.5241 - val_acc: 0.8176\n",
      "Epoch 90/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9194- ETA: 1s - loss: 0.1927 -  - ETA: 1s - loss: 0.1928 -  - ETA: 0s - loss: 0.1928 - acc: 0\n",
      "Epoch 00090: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.1929 - acc: 0.9194 - val_loss: 0.5339 - val_acc: 0.8166\n",
      "Epoch 91/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.1924 - acc: 0.9198- ETA: 8s - loss: 0.1919 - - ETA: 7s - loss: 0.1919 - acc: 0.91 - ETA:  - ETA: 4s - loss: 0.1 - ETA: 3s - loss: 0.1921 -\n",
      "Epoch 00091: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.1924 - acc: 0.9198 - val_loss: 0.5312 - val_acc: 0.8161\n",
      "Epoch 92/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.92001 ETA: 11 - ETA: 9s - loss: 0.1916 - acc: 0 - ETA: 6s - loss: 0.1918 - acc: - ETA: 6s - loss: 0.1918 -  - ETA: 0s - loss: 0.1920 - acc: 0.92\n",
      "Epoch 00092: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.1920 - acc: 0.9200 - val_loss: 0.5915 - val_acc: 0.8174\n",
      "Epoch 93/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9201\n",
      "Epoch 00093: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.1915 - acc: 0.9201 - val_loss: 0.5295 - val_acc: 0.8173\n",
      "Epoch 94/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.1910 - acc: 0.9206- ETA: 0s - loss: 0.1911 - acc: \n",
      "Epoch 00094: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.1911 - acc: 0.9206 - val_loss: 0.5782 - val_acc: 0.8155\n",
      "Epoch 95/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9207\n",
      "Epoch 00095: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.1906 - acc: 0.9207 - val_loss: 0.5431 - val_acc: 0.8154\n",
      "Epoch 96/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9208\n",
      "Epoch 00096: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.1901 - acc: 0.9208 - val_loss: 0.5200 - val_acc: 0.8166\n",
      "Epoch 97/100\n",
      "1278976/1280000 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9207- ET\n",
      "Epoch 00097: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.1899 - acc: 0.9207 - val_loss: 0.5542 - val_acc: 0.8161\n",
      "Epoch 98/100\n",
      "1279488/1280000 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9208- ETA: 0s - loss: 0.1900 - acc: 0.\n",
      "Epoch 00098: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 52us/sample - loss: 0.1900 - acc: 0.9208 - val_loss: 0.5361 - val_acc: 0.8151\n",
      "Epoch 99/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9213- ET\n",
      "Epoch 00099: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 67s 52us/sample - loss: 0.1888 - acc: 0.9213 - val_loss: 0.5934 - val_acc: 0.8159\n",
      "Epoch 100/100\n",
      "1278464/1280000 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9212\n",
      "Epoch 00100: val_acc did not improve from 0.83557\n",
      "1280000/1280000 [==============================] - 66s 51us/sample - loss: 0.1891 - acc: 0.9212 - val_loss: 0.5555 - val_acc: 0.8165\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-71b4bfcb764d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#Saving the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'final_{}_{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Twitter'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# checkpoint\n",
    "t = int(time.time())\n",
    "batch_size = 512\n",
    "\n",
    "chk_path = os.path.join(BASE_DIR, 'best_{}_{}'.format('Twitter',t))\n",
    "checkpoint = ModelCheckpoint(chk_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#tensorboard = TensorBoard(log_dir=\"logs/{}_{}\".format('IMDB_LSTM',t))\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "      batch_size=batch_size,\n",
    "      epochs=100, \n",
    "      verbose=1,\n",
    "      shuffle=True,\n",
    "      validation_data=[x_test, y_test],\n",
    "      callbacks=callbacks_list)\n",
    "\n",
    "#Saving the model\n",
    "model.save(os.path.join(BASE_DIR, 'final_{}_{}'.format('Twitter',t)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model(f'{BASE_DIR}//best_Twitter_1590447138')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[134633  25321]\n",
      " [ 27296 132750]]\n",
      "the mean-f1 score: 0.8356\n",
      "accuracy is: 0.8356\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "\n",
    "y_pred = np.rint(model.predict(x_test))\n",
    "y_true = y_test\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(cf_matrix)\n",
    "class_wise_f1 = f1_score(y_true, y_pred, average=None)\n",
    "print('the mean-f1 score: {:.4f}'.format(np.mean(class_wise_f1)))\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print('accuracy is: {:.4f}'.format(accuracy))\n",
    "\n",
    "np.save(f'{BASE_DIR}//y_test_pred', y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
